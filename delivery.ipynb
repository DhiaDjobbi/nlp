{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scrappig using BeautifulSoup (Dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sendle_com.csv already exists. Skipping scraping.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Base URL of the Trustpilot reviews page\n",
    "site_to_review = \"sendle.com\"\n",
    "base_url = \"https://www.trustpilot.com/review/\" + site_to_review + \"?page=\"\n",
    "csv_filename = site_to_review.replace('.', '_') + \".csv\"\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Function to scrape reviews from a single page\n",
    "def scrape_page(page_number):\n",
    "    url = f\"{base_url}{page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    reviews = []\n",
    "\n",
    "    review_sections = soup.find_all(\"section\", class_=\"styles_reviewContentwrapper__W9Vqf\")\n",
    "    for review in review_sections:\n",
    "        try:\n",
    "            # Extract rating\n",
    "            rating_tag = review.find(\"div\", class_=\"star-rating_starRating__sdbkn\")\n",
    "            if rating_tag and rating_tag.img and \"alt\" in rating_tag.img.attrs:\n",
    "                rating = int(rating_tag.img[\"alt\"].split()[1])\n",
    "            else:\n",
    "                rating = None\n",
    "\n",
    "            # Extract title\n",
    "            title_tag = review.find(\"h2\", class_=\"typography_heading-s__RxVny\")\n",
    "            title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "            # Extract review text\n",
    "            text_tag = review.find(\"p\", class_=\"typography_body-l__v5JLj\")\n",
    "            text = text_tag.text.strip() if text_tag else None\n",
    "\n",
    "            # Extract date\n",
    "            date_tag = review.find(\"time\")\n",
    "            date = date_tag.text.strip() if date_tag else None\n",
    "\n",
    "            # Append to reviews list\n",
    "            if rating and title and text and date:\n",
    "                reviews.append({\n",
    "                    \"rating\": rating,\n",
    "                    \"title\": title,\n",
    "                    \"text\": text,\n",
    "                    \"date\": date\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting review: {e}\")\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.exists(csv_filename):\n",
    "    print(f\"{csv_filename} already exists. Skipping scraping.\")\n",
    "else:\n",
    "    # Send a GET request to fetch the first page content\n",
    "    response = requests.get(base_url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the pagination button with name=\"pagination-button-last\"\n",
    "    pagination_button = soup.find(\"a\", {\"name\": \"pagination-button-last\"})\n",
    "    if pagination_button:\n",
    "        last_page_number = int(pagination_button[\"aria-label\"].split()[-1])\n",
    "        print(f\"Last page number: {last_page_number}\")\n",
    "    else:\n",
    "        last_page_number = 1\n",
    "\n",
    "    # List to store all reviews\n",
    "    all_reviews = []\n",
    "\n",
    "    # Scrape all pages\n",
    "    for page in range(1, last_page_number + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_reviews = scrape_page(page)\n",
    "        all_reviews.extend(page_reviews)\n",
    "\n",
    "    # Save all reviews to a CSV file\n",
    "    with open(csv_filename, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"rating\", \"title\", \"text\", \"date\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_reviews)\n",
    "\n",
    "    print(f\"Scraped {len(all_reviews)} reviews and saved to {csv_filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing reviews...\n",
      "Preprocessed data saved to sendle_com_processed.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to preprocess review text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, converting to lowercase,\n",
    "    removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    # Initialize stop words and lemmatizer\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "    # Remove stop words and lemmatize\n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "def preprocess_csv(input_csv):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, preprocesses the 'text' column, and saves the\n",
    "    preprocessed data to a new CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Check if 'text' column exists\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(\"The CSV file must have a 'text' column containing the reviews.\")\n",
    "\n",
    "    # Apply preprocessing to the 'text' column\n",
    "    print(\"Preprocessing reviews...\")\n",
    "    df[\"processed_text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    output_csv = input_csv.replace(\".csv\", \"_processed.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Preprocessed data saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input CSV file name\n",
    "    input_csv_file = \"sendle_com.csv\"\n",
    "    try:\n",
    "        preprocess_csv(input_csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sendle_com_processed_with_sentiment.csv already exists. Skipping sentiment analysis.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "def analyze_sentiment(input_csv):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment of reviews in the input CSV file using a RoBERTa model\n",
    "    and saves the results to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): Path to the input CSV file containing a 'processed_text' column.\n",
    "\n",
    "    Output:\n",
    "    - A new CSV file with a 'sentiment' column added.\n",
    "    \"\"\"\n",
    "    # Define the output CSV file name\n",
    "    output_csv = input_csv.replace(\".csv\", \"_with_sentiment.csv\")\n",
    "\n",
    "    # Check if the output CSV file already exists\n",
    "    if os.path.exists(output_csv):\n",
    "        print(f\"{output_csv} already exists. Skipping sentiment analysis.\")\n",
    "        return\n",
    "\n",
    "    # Load the input CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Check if 'processed_text' column exists\n",
    "    if \"processed_text\" not in df.columns:\n",
    "        raise ValueError(\"The CSV file must have a 'processed_text' column.\")\n",
    "\n",
    "    # Initialize the Hugging Face sentiment analysis pipeline\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "    # Map RoBERTa labels to human-readable sentiment labels\n",
    "    label_map = {\n",
    "        \"LABEL_0\": \"NEGATIVE\",\n",
    "        \"LABEL_1\": \"NEUTRAL\",\n",
    "        \"LABEL_2\": \"POSITIVE\"\n",
    "    }\n",
    "\n",
    "    # Function to analyze sentiment with error handling\n",
    "    def analyze_text(text):\n",
    "        try:\n",
    "            result = sentiment_analyzer(text)[0]\n",
    "            return label_map.get(result[\"label\"], \"UNKNOWN\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Analyze sentiment for each review\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    df[\"sentiment\"] = df[\"processed_text\"].apply(analyze_text)\n",
    "\n",
    "    # Save the results to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Sentiment analysis complete. Results saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"sendle_com_processed.csv\"\n",
    "    try:\n",
    "        analyze_sentiment(input_csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme Detection (LDA) + Vectorize TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static theme detection complete. Results saved to sendle_com_processed_with_sentiment_with_static_themes.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def detect_themes_static(input_csv):\n",
    "    # Define the static themes and their keywords\n",
    "    theme_keywords = {\n",
    "        \"Delivery Issues\": [\n",
    "            \"not picked\", \"missed\", \"failed\", \"lost\", \"late\", \"delayed\", \"undelivered\", \"error\", \n",
    "            \"rescheduled\", \"problem\", \"loss\", \"stolen\", \"never arrived\", \"never received\", \n",
    "            \"no pick up\", \"re-schedule\", \"never showed\", \"missed pickup\", \"missed delivery\",\n",
    "            \"failed delivery\", \"lost parcel\", \"late delivery\", \"delayed delivery\", \"undelivered parcel\",\n",
    "            \"parcel pickup\", \"pickup delay\", \"pickup failed\", \"not picked up\",\n",
    "            \"address wrong\", \"wrong address\", \"wrong location\", \"wrong recipient\", \"wrong recipient\",\n",
    "            \"disappeared\", \"disappearance\", \"vanished\", \"vanishing\", \"gone\", \"missing\", \"missing parcel\", \"disspear\"\n",
    "        ],\n",
    "        \"Poor Customer Service\": [\n",
    "            \"poor\", \"no response\", \"unhelpful\", \"rude\", \"bad\", \"terrible\", \"support\", \"ignored\", \n",
    "            \"unresponsive\", \"frustrating\", \"lack\", \"disappointing\", \"no manager\", \n",
    "            \"manager never called\", \"follow up\", \"continuous problems\", \"zero contact\",\n",
    "            \"no customer service\", \"no customer support\", \"no customer care\", \"no customer help\",\n",
    "            \"unhelpful response\", \"refuse to tell\", \"vague response\", \"poor service\",\"useless\",\"customer service is worse\",\"no sign\"\n",
    "        ],\n",
    "        \"Unreliable Service\": [\n",
    "            \"unreliable\", \"failed\", \"chaotic\", \"inconsistent\", \"unpredictable\", \"untrustworthy\", \n",
    "            \"unstable\", \"erratic\", \"spotty\", \"hit\", \"miss\", \"undependable\", \"unreliability\",\n",
    "            \"unreliable service\", \"unreliable company\", \"unreliable delivery\", \"unreliable courier\",\n",
    "            \"ripoff\", \"failed to deliver\", \"not shipped\", \"never arrived\", \"delayed delivery\"\n",
    "        ],\n",
    "        \"Lack of Communication\": [\n",
    "            \"no updates\", \"no response\", \"no info\", \"lack\", \"silent\", \"uncommunicative\", \"no feedback\", \n",
    "            \"no follow\", \"no status\", \"no clarity\", \"no notification\", \"no confirmation\", \"no tracking\",\n",
    "            \"no communication\", \"no contact\", \"no call\", \"no email\", \"no message\", \"no text\", \"no reply\",\n",
    "            \"unresponsive\", \"ignored\", \"unhelpful\", \"unreliable\", \"unprofessional\", \"untrustworthy\",\n",
    "            \"poor communication\", \"poor response\", \"poor feedback\", \"poor follow\", \"poor status\"\n",
    "        ],\n",
    "        \"Driver Problems\": [\n",
    "            \"driver\", \"rude\", \"late\", \"no show\", \"unprofessional\", \"lost\", \"careless\", \"aggressive\", \n",
    "            \"negligent\", \"confused\", \"error\", \"incompetent\", \"reckless\", \"unreliable\", \n",
    "            \"never shows\", \"no show driver\", \"did not show up\", \"failed pick up\",\n",
    "            \"driver never arrived\", \"driver never showed\", \"driver never came\", \"driver never picked up\",\n",
    "            \"swearing\", \"swore\", \"sworn\", \"swear\", \"cursing\", \"cursed\", \"curse\", \"cussing\", \"cussed\",\n",
    "            \"crash\", \"crashed\", \"crashing\", \"accident\", \"accidental\", \"accidentally\", \"collision\",\"No one came\"\n",
    "        ],\n",
    "        \"Parcel Handling Problems\": [\n",
    "            \"lost\", \"destroyed\", \"returned\", \"damaged\", \"missing\", \"broken\", \"opened\", \"stolen\", \n",
    "            \"misplaced\", \"mishandled\", \"crushed\", \"wet\", \"torn\", \"tampered\", \"ruined\", \"spoiled\",\n",
    "            \"mangled\", \"smashed\", \"dented\", \"cracked\", \"scratched\", \"shattered\", \"crumpled\", \"soaked\",\n",
    "            \"smash\", \"crush\", \"crumple\", \"soak\", \"soaked\", \"smashed\", \"crushed\", \"crumpled\", \"soaked\",\n",
    "            \"thrown\", \"throw\", \"toss\", \"tossed\", \"throwing\", \"tossing\", \"throw away\", \"toss away\",\n",
    "            \"throwing away\", \"tossing away\", \"throw out\", \"toss out\", \"throwing out\", \"tossing out\",\"curbside\"\n",
    "        ],\n",
    "        \"Cost and Value Concerns\": [\n",
    "            \"affordable\", \"expensive\", \"overpriced\", \"costly\", \"pricey\", \"cheap\", \"unreasonable\", \n",
    "            \"value\", \"fees\", \"hidden\", \"charges\", \"expensive\", \"money\", \"waste\", \"budget\", \"pricing\",\n",
    "            \"reasonable\", \"inexpensive\", \"cost-effective\", \"cost-efficient\", \"cost-saving\", \"charge\"\n",
    "        ],\n",
    "        \"Convenience and Ease\": [\n",
    "            \"easy\", \"convenient\", \"simple\", \"smooth\", \"quick\", \"efficient\", \"accessible\", \n",
    "            \"flexible\", \"intuitive\", \"hassle\", \"user-friendly\", \"straightforward\", \"seamless\",\"Ease\"\n",
    "        ],\n",
    "        \"Timeliness and Speed\": [\n",
    "            \"delayed\", \"late\", \"slow\", \"fast\", \"timely\", \"prompt\", \"speedy\", \"on time\", \n",
    "            \"expedited\", \"rush\", \"quick\", \"efficient\", \"time-sensitive\", \"punctual\", \"swift\",\n",
    "            \"timeframe\", \"time\", \"time-consuming\", \"time-wasting\", \"time-management\", \"time-saving\",\n",
    "            \"time-efficient\", \"time-critical\", \"time-sensitive\", \"time-consuming\", \"time-wasting\",\n",
    "            \"wait\", \"waiting\", \"waiting time\", \"waiting period\", \"waiting list\", \"waiting room\",\n",
    "            \"waiting area\", \"waiting line\", \"waiting game\", \"waiting period\", \"waiting time\",\"after\",\"takes days\",\"days\",\"how quickly\"\n",
    "        ],\n",
    "        \"Service Variability\": [\n",
    "            \"varies\", \"inconsistent\", \"unpredictable\", \"spotty\", \"mixed\", \"hit\", \"miss\", \n",
    "            \"dependent\", \"disparity\", \"uneven\", \"fluctuating\", \"varying\", \"inconclusive\", \"inconclusive\"\n",
    "        ],\n",
    "        \"Frustration and Stress\": [\n",
    "            \"frustrating\", \"stressful\", \"annoying\", \"irritating\", \"infuriating\", \"maddening\", \n",
    "            \"exasperating\", \"disappointing\", \"upsetting\", \"aggravating\",\n",
    "            \"stress\", \"frustration\", \"disappointment\",\"worst\", \"bad\", \"terrible\", \"horrible\", \"awful\",\n",
    "            \"hate\", \"dislike\", \"displeased\", \"unhappy\", \"regret\", \"regretful\",\n",
    "            \"angry\", \"anger\", \"mad\", \"madness\", \"irritated\", \"irritation\", \"annoyed\", \"annoyance\",\n",
    "            \"shocking\",\"Dont use\",\"ridiculous\",\"No word\",\"never shop\",\"t use\",\"joke\",\"fooled\"\n",
    "        ],\n",
    "        \"Shipping Issues\": [\n",
    "            \"picked up\", \"dropped off\", \"pickup\", \"drop off\", \"pickup failed\", \"parcel\", \"not picked\", \n",
    "            \"pick-up error\", \"late pickup\", \"missing pickup\", \"not picked up\", \"drop off issue\", \n",
    "            \"high volume\", \"pick-up delay\", \"never picked up\",\"outsourced my packages\",\"haven't received\",\"delivered never\",\"never got scanned\"\n",
    "        ],\n",
    "        \"Positive Experiences\": [\n",
    "            \"amazing\", \"great\", \"fantastic\", \"wonderful\", \"satisfying\", \"pleasant\", \n",
    "            \"impressive\", \"awesome\", \"top-notch\", \"smooth\", \"outstanding\", \"worth\", \"happy\", \"pleased\",\n",
    "            \"excellent\", \"love\", \"recommend\", \"recommendation\", \"recommendable\",\n",
    "            \"joy\", \"joyful\", \"satisfied\", \"satisfaction\", \"happy\", \"happiness\", \"pleased\", \"pleasure\",\n",
    "            \"impressed\", \"impressive\", \"awesome\", \"awesomeness\", \"top-notch\", \"outstanding\", \"worth\",\"that's all I need\",\n",
    "            \"excellent\", \"is good\", \"love\", \"recommend\", \"recommendation\", \"recommendable\",\"quickly\",\"helped\",\"perfect\",\"as ordered\",\"Thank you\",\"good price\"\n",
    "            \"worked hard\", \"help resolve issues\", \"is helpful\", \"helpful staff\", \"helpful team\",\"Good\",\"wont use any other\",\"fair price\",\"sweet\",\"perfect\",\"Easier to use\",\"was then able\"\n",
    "        ],\n",
    "        \"Positive Feedback\": [\n",
    "            \"amazing\", \"great\", \"fantastic\", \"wonderful\", \"satisfying\", \"pleased\", \"impressive\", \n",
    "            \"awesome\", \"top-notch\", \"outstanding\", \"happy\", \"worth\", \"excellent\", \"is good\", \"best\", \"love\",\"incredible\",\"Reliable\",\"friendly \",\"well\",\"Darling\",\"Accurate\"\n",
    "        ],\n",
    "        \"Negative Experiences\": [\n",
    "            \"frustrating\", \"infuriating\", \"upsetting\", \"terrible\", \"awful\", \"horrible\", \"worst\", \"dislike\", \n",
    "            \"hate\", \"displeased\", \"unhappy\", \"disappointment\", \"stress\", \"maddening\", \"irritating\", \n",
    "            \"annoying\", \"exasperating\",\"scam\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Define the output CSV file name\n",
    "    output_csv = input_csv.replace(\".csv\", \"_with_static_themes.csv\")\n",
    "\n",
    "    # Check if the output CSV file already exists\n",
    "    if os.path.exists(output_csv):\n",
    "        print(f\"{output_csv} already exists. Skipping theme detection.\")\n",
    "        return\n",
    "\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Combine 'title' and 'text' columns with a space\n",
    "    df[\"combined_text\"] = df[\"text\"].fillna(\"\")\n",
    "\n",
    "    # Handle missing values in the 'combined_text' column\n",
    "    df[\"combined_text\"] = df[\"combined_text\"].fillna(\"\")\n",
    "\n",
    "    # Initialize a list to store the detected themes for each review\n",
    "    detected_themes = []\n",
    "\n",
    "    # Iterate through each review and assign the most relevant theme\n",
    "    for text in df[\"combined_text\"]:\n",
    "        theme_detected = \"No Theme Detected\"  # Default theme\n",
    "        for theme, keywords in theme_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                # Use regex to match whole words only\n",
    "                if re.search(r'\\b' + re.escape(keyword) + r'\\b', text, flags=re.IGNORECASE):\n",
    "                    theme_detected = theme\n",
    "                    break  # Stop checking other keywords for this theme\n",
    "            if theme_detected != \"No Theme Detected\":\n",
    "                break  # Stop checking other themes once a theme is detected\n",
    "        detected_themes.append(theme_detected)\n",
    "\n",
    "    # Add the detected themes to the DataFrame\n",
    "    df[\"theme\"] = detected_themes\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Static theme detection complete. Results saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"sendle_com_processed_with_sentiment.csv\"\n",
    "\n",
    "    try:\n",
    "        detect_themes_static(input_csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
